{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047101aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from loguru import logger\n",
    "from poke_env.data import GenData\n",
    "from poke_env.player import MaxBasePowerPlayer, Player, RandomPlayer, SimpleHeuristicsPlayer\n",
    "\n",
    "from params import MLFLOW_EXPERIMENT_NAME, MLFLOW_URI\n",
    "from registry import load_model, save_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f0225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/124468239741862981', creation_time=1757094480281, experiment_id='124468239741862981', last_update_time=1757094480281, lifecycle_stage='active', name='PYC-AI-CHU', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc4bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3df11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca58732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, s, a, r, s2, done):\n",
    "        self.buffer.append((s, a, r, s2, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(s, dtype=torch.float32),\n",
    "            torch.tensor(a, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.float32),\n",
    "            torch.tensor(s2, dtype=torch.float32),\n",
    "            torch.tensor(d, dtype=torch.float32),\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f689a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLAgent(Player):\n",
    "    observation_space = Box(low=-1.0, high=4.0, shape=(10,), dtype=np.float32)\n",
    "    def __init__(self, battle_format, q_net, buffer, **kwargs):\n",
    "        super().__init__(battle_format=battle_format, **kwargs)\n",
    "        self.q_net = q_net\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    def embed_battle(self, battle):\n",
    "        moves_base_power = -np.ones(4, dtype=np.float32)\n",
    "        moves_dmg_multiplier = np.ones(4, dtype=np.float32)\n",
    "        type_chart = GenData.from_gen(9).type_chart  # For Gen 9 format\n",
    "\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = move.base_power / 100\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        fainted_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_opponent = len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "\n",
    "        return np.concatenate(\n",
    "            [moves_base_power, moves_dmg_multiplier, [fainted_team, fainted_opponent]],\n",
    "            axis=0\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(battle,\n",
    "                                            fainted_value=2,\n",
    "                                            hp_value=1,\n",
    "                                            victory_value=30)\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        obs = torch.tensor(self.embed_battle(battle), dtype=torch.float32)\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.choose_random_move(battle)\n",
    "        with torch.no_grad():\n",
    "            q = self.q_net(obs.unsqueeze(0))\n",
    "        action = q.argmax(dim=1).item()\n",
    "        return self._action_to_move(action, battle)\n",
    "\n",
    "    def choose_switch(self, battle):\n",
    "        obs = torch.tensor(self.embed_battle(battle), dtype=torch.float32)\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.choose_random_move(battle)\n",
    "        with torch.no_grad():\n",
    "            q = self.q_net(obs.unsqueeze(0))\n",
    "        action = q.argmax(dim=1).item()\n",
    "        return self._action_to_move(action, battle)\n",
    "\n",
    "    def _action_to_move(self, action, battle):\n",
    "        options = battle.available_moves + battle.available_switches\n",
    "        move = options[action % len(options)]\n",
    "        return self.create_order(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff91fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(agent, target_net, buffer, optimizer,\n",
    "                batch_size: int = 64, gamma: float = 0.99,\n",
    "                eps_start: float = 1.0, eps_end: float = 0.05, eps_decay: float = 1e-4,\n",
    "                update_target_every: int = 1000):\n",
    "    mlflow.log_params({\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 1e-3\n",
    "        })\n",
    "    # Decrease epsilon (linear decay)\n",
    "    agent.epsilon = max(eps_end, agent.epsilon - eps_decay)\n",
    "\n",
    "    # Skip training if buffer is too small\n",
    "    if len(buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    s, a, r, s2, done = buffer.sample(batch_size)\n",
    "    # Predictions for current states\n",
    "    q_values = agent.q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute target values using target network\n",
    "    with torch.no_grad():\n",
    "        max_next_q = target_net(s2).max(1)[0]\n",
    "        targets = r + gamma * (1 - done) * max_next_q\n",
    "\n",
    "    # Compute loss and update network\n",
    "    loss = F.mse_loss(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Periodically sync target network\n",
    "    train_step = getattr(agent, \"train_step_count\", 0) + 1\n",
    "    agent.train_step_count = train_step\n",
    "    if train_step % update_target_every == 0:\n",
    "        target_net.load_state_dict(agent.q_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080e888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "output_dim = 26  # discrete number of possible moves\n",
    "q_net = QNetwork(input_dim, output_dim)\n",
    "target_net = QNetwork(input_dim, output_dim)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "target_net = target_net.cuda()\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "buffer = ReplayBuffer(capacity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ead1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleRLAgent(battle_format=\"gen9randombattle\", q_net=q_net, buffer=buffer)\n",
    "opponent = RandomPlayer(battle_format=\"gen9randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12efcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(agent, target_net, buffer, optimizer, batch_size=64, gamma=0.99):\n",
    "    if len(buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    s, a, r, s2, done = buffer.sample(batch_size)\n",
    "    q_values = agent.q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_next_q = target_net(s2).max(1)[0]\n",
    "        targets = r + gamma * (1 - done) * max_next_q\n",
    "\n",
    "    loss = F.mse_loss(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def train_agent():\n",
    "    for epoch in range(10):  # 10 epochs\n",
    "        print(f\"Epoch {epoch + 1} started.\")\n",
    "\n",
    "        for battle_num in range(1):  # 1000 battles per epoch\n",
    "            print(f\"  Battle {battle_num + 1} started.\")\n",
    "            battle = await agent.battle_against(\n",
    "                opponent,\n",
    "                n_battles=1\n",
    "            )\n",
    "            # Perform training after each battle\n",
    "            train_step(agent, target_net, buffer, optimizer)\n",
    "            print(f\"  Battle {battle_num + 1} finished.\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "310b8fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 1 finished.\n",
      "Epoch 2 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 2 finished.\n",
      "Epoch 3 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 3 finished.\n",
      "Epoch 4 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 4 finished.\n",
      "Epoch 5 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 5 finished.\n",
      "Epoch 6 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 6 finished.\n",
      "Epoch 7 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 7 finished.\n",
      "Epoch 8 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 8 finished.\n",
      "Epoch 9 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 9 finished.\n",
      "Epoch 10 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 10 finished.\n"
     ]
    }
   ],
   "source": [
    "await train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c605a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-05 19:48:17.182\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mPlayer SimpleRLAgent 1 won 6 out of 10 played\u001b[0m\n",
      "2025/09/05 19:48:20 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7398120fa690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_won_battles and n_finished_battles\n",
    "mlflow.log_metrics({'win_ratio':agent.n_won_battles/agent.n_finished_battles})\n",
    "logger.debug(\n",
    "    f\"Player {agent.username} won {agent.n_won_battles} out of {\\\n",
    "        agent.n_finished_battles} played\"\n",
    ")\n",
    "\n",
    "mlflow.pytorch.log_model(\n",
    "        pytorch_model=agent.q_net,\n",
    "        name=\"dqn_model\",\n",
    "        input_example=np.zeros((1, 1, 10), dtype=np.float32)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
