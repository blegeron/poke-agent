{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047101aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from loguru import logger\n",
    "from poke_env.data import GenData\n",
    "from poke_env.player import MaxBasePowerPlayer, Player, RandomPlayer, SimpleHeuristicsPlayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc4bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3df11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca58732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, s, a, r, s2, done):\n",
    "        self.buffer.append((s, a, r, s2, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(s, dtype=torch.float32),\n",
    "            torch.tensor(a, dtype=torch.long),\n",
    "            torch.tensor(r, dtype=torch.float32),\n",
    "            torch.tensor(s2, dtype=torch.float32),\n",
    "            torch.tensor(d, dtype=torch.float32),\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f689a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLAgent(Player):\n",
    "    observation_space = Box(low=-1.0, high=4.0, shape=(10,), dtype=np.float32)\n",
    "    def __init__(self, battle_format, q_net, buffer, **kwargs):\n",
    "        super().__init__(battle_format=battle_format, **kwargs)\n",
    "        self.q_net = q_net\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "    def embed_battle(self, battle):\n",
    "        moves_base_power = -np.ones(4, dtype=np.float32)\n",
    "        moves_dmg_multiplier = np.ones(4, dtype=np.float32)\n",
    "        type_chart = GenData.from_gen(9).type_chart  # For Gen 9 format\n",
    "\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = move.base_power / 100\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        fainted_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_opponent = len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "\n",
    "        return np.concatenate(\n",
    "            [moves_base_power, moves_dmg_multiplier, [fainted_team, fainted_opponent]],\n",
    "            axis=0\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(battle,\n",
    "                                            fainted_value=2,\n",
    "                                            hp_value=1,\n",
    "                                            victory_value=30)\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        obs = torch.tensor(self.embed_battle(battle), dtype=torch.float32)\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.choose_random_move(battle)\n",
    "        with torch.no_grad():\n",
    "            q = self.q_net(obs.unsqueeze(0))\n",
    "        action = q.argmax(dim=1).item()\n",
    "        return self._action_to_move(action, battle)\n",
    "\n",
    "    def choose_switch(self, battle):\n",
    "        obs = torch.tensor(self.embed_battle(battle), dtype=torch.float32)\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.choose_random_move(battle)\n",
    "        with torch.no_grad():\n",
    "            q = self.q_net(obs.unsqueeze(0))\n",
    "        action = q.argmax(dim=1).item()\n",
    "        return self._action_to_move(action, battle)\n",
    "\n",
    "    def _action_to_move(self, action, battle):\n",
    "        options = battle.available_moves + battle.available_switches\n",
    "        move = options[action % len(options)]\n",
    "        return self.create_order(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff91fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(agent, target_net, buffer, optimizer,\n",
    "                batch_size: int = 64, gamma: float = 0.99,\n",
    "                eps_start: float = 1.0, eps_end: float = 0.05, eps_decay: float = 1e-4,\n",
    "                update_target_every: int = 1000):\n",
    "    # Decrease epsilon (linear decay)\n",
    "    agent.epsilon = max(eps_end, agent.epsilon - eps_decay)\n",
    "\n",
    "    # Skip training if buffer is too small\n",
    "    if len(buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    s, a, r, s2, done = buffer.sample(batch_size)\n",
    "    # Predictions for current states\n",
    "    q_values = agent.q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute target values using target network\n",
    "    with torch.no_grad():\n",
    "        max_next_q = target_net(s2).max(1)[0]\n",
    "        targets = r + gamma * (1 - done) * max_next_q\n",
    "\n",
    "    # Compute loss and update network\n",
    "    loss = F.mse_loss(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Periodically sync target network\n",
    "    train_step = getattr(agent, \"train_step_count\", 0) + 1\n",
    "    agent.train_step_count = train_step\n",
    "    if train_step % update_target_every == 0:\n",
    "        target_net.load_state_dict(agent.q_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "output_dim = 26  # discrete number of possible moves\n",
    "q_net = QNetwork(input_dim, output_dim)\n",
    "target_net = QNetwork(input_dim, output_dim)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "target_net = target_net.cuda()\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "buffer = ReplayBuffer(capacity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ead1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleRLAgent(battle_format=\"gen9randombattle\", q_net=q_net, buffer=buffer)\n",
    "opponent = RandomPlayer(battle_format=\"gen9randombattle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12efcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(agent, target_net, buffer, optimizer, batch_size=64, gamma=0.99):\n",
    "    if len(buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    s, a, r, s2, done = buffer.sample(batch_size)\n",
    "    q_values = agent.q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_next_q = target_net(s2).max(1)[0]\n",
    "        targets = r + gamma * (1 - done) * max_next_q\n",
    "\n",
    "    loss = F.mse_loss(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def train_agent():\n",
    "    for epoch in range(10):  # 10 epochs\n",
    "        print(f\"Epoch {epoch + 1} started.\")\n",
    "\n",
    "        for battle_num in range(1):  # 1000 battles per epoch\n",
    "            print(f\"  Battle {battle_num + 1} started.\")\n",
    "            battle = await agent.battle_against(\n",
    "                opponent,\n",
    "                n_battles=1\n",
    "            )\n",
    "            # Perform training after each battle\n",
    "            train_step(agent, target_net, buffer, optimizer)\n",
    "            print(f\"  Battle {battle_num + 1} finished.\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310b8fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 1 finished.\n",
      "Epoch 2 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 2 finished.\n",
      "Epoch 3 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 3 finished.\n",
      "Epoch 4 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 4 finished.\n",
      "Epoch 5 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 5 finished.\n",
      "Epoch 6 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 6 finished.\n",
      "Epoch 7 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 7 finished.\n",
      "Epoch 8 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 8 finished.\n",
      "Epoch 9 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 9 finished.\n",
      "Epoch 10 started.\n",
      "  Battle 1 started.\n",
      "  Battle 1 finished.\n",
      "Epoch 10 finished.\n"
     ]
    }
   ],
   "source": [
    "await train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c605a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-05 18:24:30.753\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34m\u001b[1mPlayer SimpleRLAgent 1 won 7 out of 10 played\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# n_won_battles and n_finished_battles\n",
    "\n",
    "logger.debug(\n",
    "    f\"Player {agent.username} won {agent.n_won_battles} out of {\\\n",
    "        agent.n_finished_battles} played\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2efcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.weight',\n",
       "              tensor([[ 0.0279,  0.1536, -0.0660,  ...,  0.1591, -0.0409,  0.2534],\n",
       "                      [-0.2426, -0.3127, -0.0439,  ...,  0.1780,  0.3155,  0.0996],\n",
       "                      [-0.2746, -0.0578,  0.0894,  ..., -0.1347,  0.1217, -0.1539],\n",
       "                      ...,\n",
       "                      [ 0.2579, -0.1498,  0.0201,  ..., -0.1425,  0.1439, -0.1245],\n",
       "                      [ 0.0502, -0.2218,  0.1814,  ..., -0.0690, -0.0927,  0.1766],\n",
       "                      [-0.2531,  0.1493,  0.3080,  ...,  0.1480,  0.0216, -0.1109]])),\n",
       "             ('net.0.bias',\n",
       "              tensor([ 0.2198,  0.2221,  0.3156, -0.0919, -0.3018, -0.0477, -0.1609, -0.1056,\n",
       "                      -0.0814, -0.0918, -0.0048, -0.1039,  0.2363,  0.2775, -0.1792, -0.1546,\n",
       "                       0.1221,  0.0520,  0.1126, -0.0255, -0.0644, -0.1394,  0.2379,  0.3114,\n",
       "                       0.0159,  0.2154, -0.0923, -0.1288,  0.2592,  0.0886,  0.1518, -0.1153,\n",
       "                      -0.2030,  0.1807,  0.2631, -0.0812, -0.2551, -0.2900, -0.0304, -0.0124,\n",
       "                       0.0285,  0.0012, -0.2694, -0.1328,  0.0339,  0.2759, -0.2584,  0.2043,\n",
       "                       0.2244,  0.1055, -0.1244,  0.2530,  0.1034, -0.1310,  0.0367, -0.0195,\n",
       "                       0.2464, -0.1670, -0.0606, -0.0593,  0.2684,  0.1282,  0.0462, -0.0466,\n",
       "                      -0.2914, -0.0657, -0.2124,  0.2304,  0.0453,  0.1777,  0.2177, -0.0273,\n",
       "                      -0.1253,  0.3043, -0.1503,  0.0527,  0.2522,  0.0750,  0.2393,  0.2227,\n",
       "                       0.3162, -0.1826,  0.2377, -0.2587, -0.0910, -0.0446,  0.1273, -0.1105,\n",
       "                       0.2501, -0.3105,  0.2842,  0.0959,  0.2625,  0.0793, -0.1073, -0.0124,\n",
       "                       0.2735, -0.2569, -0.1034,  0.1314, -0.0308,  0.2310, -0.1968,  0.1812,\n",
       "                       0.0252,  0.1113,  0.1446, -0.0288, -0.1622, -0.2730, -0.1637, -0.0357,\n",
       "                       0.1491,  0.1741, -0.2088, -0.0057,  0.2691,  0.1975, -0.0508,  0.1158,\n",
       "                       0.2716,  0.1918, -0.1946,  0.0895, -0.2403,  0.0364,  0.0414,  0.0946])),\n",
       "             ('net.2.weight',\n",
       "              tensor([[-0.0610, -0.0311, -0.0577,  ...,  0.0439, -0.0136, -0.0772],\n",
       "                      [ 0.0642, -0.0044, -0.0679,  ..., -0.0840, -0.0458, -0.0081],\n",
       "                      [-0.0082, -0.0113,  0.0465,  ...,  0.0021,  0.0763,  0.0371],\n",
       "                      ...,\n",
       "                      [-0.0131,  0.0851,  0.0284,  ...,  0.0706,  0.0366, -0.0455],\n",
       "                      [ 0.0339, -0.0606, -0.0611,  ..., -0.0383,  0.0826,  0.0030],\n",
       "                      [ 0.0615,  0.0394, -0.0391,  ..., -0.0798, -0.0726, -0.0802]])),\n",
       "             ('net.2.bias',\n",
       "              tensor([ 0.0183, -0.0648, -0.0764,  0.0439,  0.0582,  0.0577,  0.0589,  0.0453,\n",
       "                       0.0703,  0.0168, -0.0147,  0.0859,  0.0168,  0.0006,  0.0221, -0.0875,\n",
       "                       0.0075, -0.0806, -0.0818, -0.0606,  0.0002, -0.0620,  0.0740, -0.0154,\n",
       "                      -0.0557,  0.0384,  0.0089,  0.0384,  0.0010, -0.0782,  0.0280, -0.0030,\n",
       "                      -0.0576, -0.0461,  0.0799, -0.0391,  0.0674, -0.0119,  0.0596,  0.0664,\n",
       "                      -0.0083,  0.0682,  0.0545,  0.0239,  0.0006, -0.0324, -0.0507,  0.0200,\n",
       "                      -0.0753, -0.0787,  0.0651,  0.0228,  0.0388,  0.0581,  0.0180, -0.0013,\n",
       "                      -0.0036, -0.0865,  0.0701, -0.0474, -0.0538, -0.0750,  0.0519, -0.0191])),\n",
       "             ('net.4.weight',\n",
       "              tensor([[-0.0645, -0.1057,  0.0931,  ...,  0.0730, -0.0375,  0.0776],\n",
       "                      [ 0.0751,  0.0980,  0.1247,  ...,  0.0194, -0.0463,  0.0756],\n",
       "                      [-0.0778, -0.0235,  0.0552,  ..., -0.0255, -0.0247,  0.0273],\n",
       "                      ...,\n",
       "                      [ 0.1187, -0.0012,  0.0242,  ...,  0.0186, -0.1170, -0.1114],\n",
       "                      [ 0.0560,  0.0620, -0.1250,  ..., -0.0550, -0.0764, -0.0507],\n",
       "                      [ 0.0007, -0.0953,  0.1206,  ..., -0.0141,  0.0718, -0.0667]])),\n",
       "             ('net.4.bias',\n",
       "              tensor([ 0.0418,  0.0099,  0.0245, -0.0471, -0.0244, -0.0213,  0.0285,  0.0717,\n",
       "                       0.0589,  0.0825,  0.0846, -0.0877, -0.0604,  0.0694,  0.0316, -0.0331,\n",
       "                       0.0800, -0.0226, -0.0718,  0.0498,  0.0395, -0.0460,  0.0352,  0.0400,\n",
       "                       0.0233,  0.0345]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
